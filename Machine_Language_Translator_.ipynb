{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saarang2003/Machine-Tranlator/blob/main/Machine_Language_Translator_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV data (assuming your file is 'sentences.csv')\n",
        "df = pd.read_csv('eng-hin.csv', header=None)\n",
        "\n",
        "# Ensure rows with missing '-' symbol are handled gracefully\n",
        "# Split the sentence on the '-' symbol, but only if it exists\n",
        "df[['English', 'Hindi']] = df[0].str.split('-', expand=True, n=1)\n",
        "\n",
        "# Drop rows where either English or Hindi is missing\n",
        "df.dropna(subset=['English', 'Hindi'], inplace=True)\n",
        "\n",
        "# Optionally, reset the index\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "df.to_csv('cleaned_sentences.csv', index=False)\n",
        "\n",
        "# Display the cleaned dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSA2cElfNCCr",
        "outputId": "7fe5dc00-5d8f-413f-8742-b1fafb001c1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                     0  \\\n",
            "0    I am learning to drive. - मैं गाड़ी चलाना सीख ...   \n",
            "1    Can you recommend a good restaurant? - क्या आप...   \n",
            "2    I need to buy groceries. - मुझे किराने का सामा...   \n",
            "3    I love watching movies. - मुझे फिल्में देखना ब...   \n",
            "4    Where is the nearest bank? - सबसे नज़दीकी बैंक...   \n",
            "..                                                 ...   \n",
            "295         Please write it down - कृपया इसे लिख दीजिए   \n",
            "296               I need an umbrella - मुझे छाता चाहिए   \n",
            "297               This is my friend - यह मेरा दोस्त है   \n",
            "298    Do you have change? - क्या आपके पास छुट्टे हैं?   \n",
            "299                 Let's take a break - चलो ब्रेक लें   \n",
            "\n",
            "                                   English  \\\n",
            "0                 I am learning to drive.    \n",
            "1    Can you recommend a good restaurant?    \n",
            "2                I need to buy groceries.    \n",
            "3                 I love watching movies.    \n",
            "4              Where is the nearest bank?    \n",
            "..                                     ...   \n",
            "295                  Please write it down    \n",
            "296                    I need an umbrella    \n",
            "297                     This is my friend    \n",
            "298                   Do you have change?    \n",
            "299                    Let's take a break    \n",
            "\n",
            "                                           Hindi  \n",
            "0                   मैं गाड़ी चलाना सीख रहा हूँ।  \n",
            "1     क्या आप एक अच्छा रेस्टोरेंट सुझा सकते हैं?  \n",
            "2                मुझे किराने का सामान खरीदना है।  \n",
            "3               मुझे फिल्में देखना बहुत पसंद है।  \n",
            "4                     सबसे नज़दीकी बैंक कहाँ है?  \n",
            "..                                           ...  \n",
            "295                          कृपया इसे लिख दीजिए  \n",
            "296                              मुझे छाता चाहिए  \n",
            "297                             यह मेरा दोस्त है  \n",
            "298                    क्या आपके पास छुट्टे हैं?  \n",
            "299                                चलो ब्रेक लें  \n",
            "\n",
            "[300 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train SentencePiece model\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='cleaned_sentences.csv',  # Use your dataset\n",
        "    model_prefix='spm_model',\n",
        "    vocab_size=3500,  # Adjusted vocabulary size\n",
        "    character_coverage=0.9995,  # Covers Hindi, English, and special characters\n",
        "    model_type='bpe'  # Byte-Pair Encoding\n",
        ")\n"
      ],
      "metadata": {
        "id": "HoPFdsHuNE0s"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('spm_model.model')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4jCoycIeJ4e",
        "outputId": "633ce52c-8045-429f-f10f-d91bfdafd24e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Load SentencePiece model (train your own if needed)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('spm_model.model')  # Replace with your trained model path\n",
        "\n",
        "# Load CSV file\n",
        "file_path = 'cleaned_sentences.csv'  # Replace with your CSV file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Tokenization using SentencePiece\n",
        "df['English_Tokens'] = df['English'].apply(lambda x: sp.encode_as_pieces(str(x)))\n",
        "df['Hindi_Tokens'] = df['Hindi'].apply(lambda x: sp.encode_as_pieces(str(x)))\n",
        "\n",
        "# Save the tokenized data to a new CSV\n",
        "output_path = 'tokenized_file.csv'\n",
        "df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Tokenization complete! Data saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3eWl4-NOLuI",
        "outputId": "e721b303-c766-4ce7-e68d-dcbcfadd2097"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete! Data saved to tokenized_file.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocabulary size: {sp.get_piece_size()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfdqAhrKci-0",
        "outputId": "19b90501-890f-4640-d6da-66f1e88b4e72"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 3500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xB3nW4ZWgCaj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbhNl1BegVjn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load SentencePiece model (train your own if needed)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('spm_model.model')  # Replace with your trained model path\n",
        "\n",
        "# Load CSV file\n",
        "file_path = 'cleaned_sentences.csv'  # Replace with your CSV file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Tokenization using SentencePiece\n",
        "df['English_Tokens'] = df['English'].fillna('').apply(lambda x: sp.encode_as_pieces(str(x)))\n",
        "df['Hindi_Tokens'] = df['Hindi'].fillna('').apply(lambda x: sp.encode_as_pieces(str(x)))\n",
        "\n",
        "# Sentence alignment verification\n",
        "df['Alignment_Flag'] = df.apply(lambda row: abs(len(row['English_Tokens']) - len(row['Hindi_Tokens'])) > 5, axis=1)\n",
        "df_mismatch = df[df['Alignment_Flag']]\n",
        "df_mismatch.to_csv('alignment_issues.csv', index=False)\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%)\n",
        "train_data, temp_data = train_test_split(df[~df['Alignment_Flag']], test_size=0.3, random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Save split data to new CSV files\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "val_data.to_csv('val_data.csv', index=False)\n",
        "test_data.to_csv('test_data.csv', index=False)\n",
        "\n",
        "print(\"Data splitting complete! Files saved as 'train_data.csv', 'val_data.csv', and 'test_data.csv'.\")\n",
        "print(\"Alignment issues saved as 'alignment_issues.csv' for manual inspection.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDnna8GThVTo",
        "outputId": "c87e86ec-290a-4b7a-84c9-54d68abf9cd7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data splitting complete! Files saved as 'train_data.csv', 'val_data.csv', and 'test_data.csv'.\n",
            "Alignment issues saved as 'alignment_issues.csv' for manual inspection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install indic-nlp-library\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23mNnFYuiMIv",
        "outputId": "045ef0c8-d782-4c72-a383-0e6ccec87a61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.1)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.1.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.1.31)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IohIYOUaiVYZ",
        "outputId": "4e473b96-e6cf-44e8-8fb1-3018c464f719"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep_translator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBLlIC38i6TH",
        "outputId": "73fdbc8f-52bc-4663-bb80-508f582c6ab2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep_translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2025.1.31)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep_translator\n",
            "Successfully installed deep_translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from deep_translator import GoogleTranslator\n",
        "import re\n",
        "\n",
        "# Download WordNet if not available\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Improved English augmentation logic\n",
        "def augment_english(text):\n",
        "    words = text.split()\n",
        "    augmented_text = []\n",
        "\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "        if synonyms:\n",
        "            synonym_words = [lem.name().replace('_', ' ') for syn in synonyms for lem in syn.lemmas()]\n",
        "            valid_synonyms = [w for w in synonym_words if re.match(r'^[a-zA-Z]+$', w)]\n",
        "            if valid_synonyms:\n",
        "                replacement = random.choice(valid_synonyms)\n",
        "                augmented_text.append(replacement)\n",
        "            else:\n",
        "                augmented_text.append(word)\n",
        "        else:\n",
        "            augmented_text.append(word)\n",
        "\n",
        "    return ' '.join(augmented_text)\n",
        "\n",
        "# Hindi augmentation with transliteration\n",
        "def augment_hindi(text):\n",
        "    return GoogleTranslator(source='hi', target='en').translate(text)\n",
        "\n",
        "# Punctuation cleanup\n",
        "def clean_punctuation(text):\n",
        "    return re.sub(r'(?<!\\w)([.,!?])', r' \\1', text).replace(' .', '.').replace(' ,', ',').replace(' ?', '?').replace(' !', '!')\n",
        "\n",
        "# Sample Data\n",
        "english_sentences = [\n",
        "    \"I want to visit a temple.\",\n",
        "    \"I am interested in photography.\",\n",
        "    \"Can I borrow your phone charger?\",\n",
        "]\n",
        "\n",
        "hindi_sentences = [\n",
        "    \"मुझे एक मंदिर जाना है।\",\n",
        "    \"मुझे फोटोग्राफी में रुचि है।\",\n",
        "    \"क्या मैं आपका फोन चार्जर उधार ले सकता हूँ?\",\n",
        "]\n",
        "\n",
        "# Process English and Hindi Augmentations\n",
        "for eng, hin in zip(english_sentences, hindi_sentences):\n",
        "    print(f\"Original English: {eng}\")\n",
        "    print(f\"Augmented English: {clean_punctuation(augment_english(eng))}\")\n",
        "    print(f\"Original Hindi: {hin}\")\n",
        "    print(f\"Augmented Hindi: {clean_punctuation(augment_hindi(hin))}\")\n",
        "    print('-' * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk1YERgVhnYz",
        "outputId": "bfa72ea5-4bca-4686-a489-4c5901d0124e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original English: I want to visit a temple.\n",
            "Augmented English: I require to visit A temple.\n",
            "Original Hindi: मुझे एक मंदिर जाना है।\n",
            "Augmented Hindi: I have to go to a temple.\n",
            "--------------------------------------------------\n",
            "Original English: I am interested in photography.\n",
            "Augmented English: i personify interested in photography.\n",
            "Original Hindi: मुझे फोटोग्राफी में रुचि है।\n",
            "Augmented Hindi: I am interested in photography.\n",
            "--------------------------------------------------\n",
            "Original English: Can I borrow your phone charger?\n",
            "Augmented English: can i borrow your headphone charger?\n",
            "Original Hindi: क्या मैं आपका फोन चार्जर उधार ले सकता हूँ?\n",
            "Augmented Hindi: Can I borrow your phone charger?\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"spm_model.model\")\n",
        "\n",
        "# Function to tokenize and convert sentences to token IDs\n",
        "def tokenize_text(text, pad_id=0, max_length=50):\n",
        "    token_ids = sp.encode(text, out_type=int)\n",
        "    padded_token_ids = token_ids[:max_length] + [pad_id] * max(0, max_length - len(token_ids))\n",
        "    mask = [1] * len(token_ids[:max_length]) + [0] * max(0, max_length - len(token_ids))\n",
        "    return padded_token_ids, mask\n",
        "\n",
        "# Process data files\n",
        "def process_file(file_path, output_file):\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Assume columns: 'English' and 'Hindi' exist\n",
        "    data[['English_Token_Ids', 'English_Mask']] = data['English'].apply(lambda x: pd.Series(tokenize_text(str(x))))\n",
        "    data[['Hindi_Token_Ids', 'Hindi_Mask']] = data['Hindi'].apply(lambda x: pd.Series(tokenize_text(str(x))))\n",
        "\n",
        "    # Save the processed data\n",
        "    data.to_csv(output_file, index=False)\n",
        "    print(f\"Processed data saved to {output_file}\")\n",
        "\n",
        "# Process train, validation, and test data\n",
        "process_file('train_data.csv', 'train_tokenized.csv')\n",
        "process_file('val_data.csv', 'val_tokenized.csv')\n",
        "process_file('test_data.csv', 'test_tokenized.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8rU-0hRh-8P",
        "outputId": "dce35071-69a7-49b5-c37c-160ed6855a3e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed data saved to train_tokenized.csv\n",
            "Processed data saved to val_tokenized.csv\n",
            "Processed data saved to test_tokenized.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('train_tokenized.csv')\n",
        "\n",
        "# Display first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5RPxq7ltZg",
        "outputId": "11428bf6-e6e2-4092-cee1-7be8f572ee6c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   0  \\\n",
            "0  Do you speak English? - क्या आप अंग्रेज़ी बोलत...   \n",
            "1  I want to visit a temple. - मुझे एक मंदिर जाना...   \n",
            "2                         Let's go now - चलो अब चलें   \n",
            "3  I am interested in photography. - मुझे फोटोग्र...   \n",
            "4  Can I borrow your phone charger? - क्या मैं आप...   \n",
            "\n",
            "                             English  \\\n",
            "0             Do you speak English?    \n",
            "1         I want to visit a temple.    \n",
            "2                      Let's go now    \n",
            "3   I am interested in photography.    \n",
            "4  Can I borrow your phone charger?    \n",
            "\n",
            "                                         Hindi  \\\n",
            "0                 क्या आप अंग्रेज़ी बोलते हैं?   \n",
            "1                       मुझे एक मंदिर जाना है।   \n",
            "2                                  चलो अब चलें   \n",
            "3                 मुझे फोटोग्राफी में रुचि है।   \n",
            "4   क्या मैं आपका फोन चार्जर उधार ले सकता हूँ?   \n",
            "\n",
            "                                      English_Tokens  \\\n",
            "0         ['▁Do', '▁you', '▁speak', '▁English', '?']   \n",
            "1  ['▁I', '▁want', '▁to', '▁visit', '▁a', '▁templ...   \n",
            "2                  ['▁Let', \"'\", 's', '▁go', '▁now']   \n",
            "3  ['▁I', '▁am', '▁interested', '▁in', '▁photogra...   \n",
            "4  ['▁Can', '▁I', '▁borrow', '▁your', '▁phone', '...   \n",
            "\n",
            "                                        Hindi_Tokens  Alignment_Flag  \\\n",
            "0  ['▁क्या', '▁आप', '▁अंग्रेज़ी', '▁बोलते', '▁हैं...           False   \n",
            "1    ['▁मुझे', '▁एक', '▁मंदिर', '▁जाना', '▁है', '।']           False   \n",
            "2                           ['▁चलो', '▁अब', '▁चलें']           False   \n",
            "3  ['▁मुझे', '▁फोटोग्राफी', '▁में', '▁रुचि', '▁है...           False   \n",
            "4  ['▁क्या', '▁मैं', '▁आपका', '▁फोन', '▁चार्जर', ...           False   \n",
            "\n",
            "                                   English_Token_Ids  \\\n",
            "0  [357, 47, 461, 956, 3415, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "1  [12, 209, 26, 462, 11, 1672, 3444, 0, 0, 0, 0,...   \n",
            "2  [558, 3467, 3410, 140, 320, 0, 0, 0, 0, 0, 0, ...   \n",
            "3  [12, 58, 981, 245, 1787, 3444, 0, 0, 0, 0, 0, ...   \n",
            "4  [150, 12, 937, 171, 289, 958, 3415, 0, 0, 0, 0...   \n",
            "\n",
            "                                        English_Mask  \\\n",
            "0  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "1  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "2  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "4  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "\n",
            "                                     Hindi_Token_Ids  \\\n",
            "0  [39, 51, 978, 1626, 67, 3415, 0, 0, 0, 0, 0, 0...   \n",
            "1  [16, 108, 1629, 287, 6, 3445, 0, 0, 0, 0, 0, 0...   \n",
            "2  [321, 362, 607, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
            "3  [16, 1782, 208, 891, 6, 3445, 0, 0, 0, 0, 0, 0...   \n",
            "4  [39, 25, 210, 322, 950, 605, 431, 105, 36, 341...   \n",
            "\n",
            "                                          Hindi_Mask  \n",
            "0  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "2  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "3  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1JlnqjJmHet"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('train_tokenized.csv')\n",
        "\n",
        "# Display the column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xhai5iKmWge",
        "outputId": "ac08a333-99e8-426f-efcc-8bbb462a59da"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['0', 'English', 'Hindi', 'English_Tokens', 'Hindi_Tokens',\n",
            "       'Alignment_Flag', 'English_Token_Ids', 'English_Mask',\n",
            "       'Hindi_Token_Ids', 'Hindi_Mask'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For English tokens\n",
        "sequence_lengths = df['English_Token_Ids'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "# For Hindi tokens\n",
        "sequence_lengths = df['Hindi_Token_Ids'].apply(lambda x: len(eval(x)))\n",
        "\n",
        "print(sequence_lengths.describe())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI-IiqvZm5Ma",
        "outputId": "80a222ba-0d94-4163-e411-6991f6335645"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    209.0\n",
            "mean      50.0\n",
            "std        0.0\n",
            "min       50.0\n",
            "25%       50.0\n",
            "50%       50.0\n",
            "75%       50.0\n",
            "max       50.0\n",
            "Name: Hindi_Token_Ids, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[['English_Token_Ids', 'Hindi_Token_Ids']].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_rOeuzznCdy",
        "outputId": "18b1a427-1806-463c-91b0-84873c6f509f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   English_Token_Ids  \\\n",
            "0  [357, 47, 461, 956, 3415, 0, 0, 0, 0, 0, 0, 0,...   \n",
            "1  [12, 209, 26, 462, 11, 1672, 3444, 0, 0, 0, 0,...   \n",
            "2  [558, 3467, 3410, 140, 320, 0, 0, 0, 0, 0, 0, ...   \n",
            "3  [12, 58, 981, 245, 1787, 3444, 0, 0, 0, 0, 0, ...   \n",
            "4  [150, 12, 937, 171, 289, 958, 3415, 0, 0, 0, 0...   \n",
            "\n",
            "                                     Hindi_Token_Ids  \n",
            "0  [39, 51, 978, 1626, 67, 3415, 0, 0, 0, 0, 0, 0...  \n",
            "1  [16, 108, 1629, 287, 6, 3445, 0, 0, 0, 0, 0, 0...  \n",
            "2  [321, 362, 607, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "3  [16, 1782, 208, 891, 6, 3445, 0, 0, 0, 0, 0, 0...  \n",
            "4  [39, 25, 210, 322, 950, 605, 431, 105, 36, 341...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerTranslator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, emb_dim=256, n_heads=8, n_layers=4, dropout=0.1):\n",
        "        super(TransformerTranslator, self).__init__()\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dropout=dropout),\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=emb_dim, nhead=n_heads, dropout=dropout),\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        self.src_embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.tgt_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.output_layer = nn.Linear(emb_dim, output_dim)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src_emb = self.src_embedding(src)\n",
        "        tgt_emb = self.tgt_embedding(tgt)\n",
        "\n",
        "        encoder_output = self.encoder(src_emb, src_mask)\n",
        "        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask)\n",
        "\n",
        "        return self.output_layer(decoder_output)\n"
      ],
      "metadata": {
        "id": "KwgcQBKonQbs"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_dim, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, emb_dim)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, emb_dim, 2) * -(torch.log(torch.tensor(10000.0)) / emb_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n"
      ],
      "metadata": {
        "id": "yaJPtLozoiga"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(column):\n",
        "    vocab_counter = Counter(token for tokens in column for token in tokens)\n",
        "    vocab = {word: idx for idx, (word, _) in enumerate(vocab_counter.most_common())}\n",
        "\n",
        "    # Add special tokens\n",
        "    vocab['<PAD>'] = len(vocab)\n",
        "    vocab['<SOS>'] = len(vocab)\n",
        "    vocab['<EOS>'] = len(vocab)\n",
        "\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "ShFVbYCkqPgO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vocab = build_vocab(df['English_Tokens'])\n",
        "hindi_vocab = build_vocab(df['Hindi_Tokens'])\n",
        "\n",
        "input_dim = len(english_vocab)  # Size of English vocabulary\n",
        "output_dim = len(hindi_vocab)   # Size of Hindi vocabulary\n"
      ],
      "metadata": {
        "id": "xZWZZZ6CqWxG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def encode_sentences(column, vocab):\n",
        "    return [[vocab.get(token, vocab['<PAD>']) for token in tokens] for tokens in column]\n",
        "\n",
        "df['English_Encoded'] = encode_sentences(df['English_Tokens'], english_vocab)\n",
        "df['Hindi_Encoded'] = encode_sentences(df['Hindi_Tokens'], hindi_vocab)\n"
      ],
      "metadata": {
        "id": "B01_A_nIqh7W"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = len(english_vocab)  # Size of English vocabulary\n",
        "output_dim = len(hindi_vocab)    # Size of Hindi vocabulary\n",
        "\n",
        "model = TransformerTranslator(input_dim, output_dim)\n"
      ],
      "metadata": {
        "id": "B4loKn3PqnLf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_token_id = 0\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n"
      ],
      "metadata": {
        "id": "qR5bHB4uqueq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize tokenizer (e.g., for BERT or a custom model)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # or another model of your choice\n",
        "\n",
        "# Set padding token (if not already defined)\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "\n",
        "# Now you can access the padding token ID\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "print(f\"Padding Token ID: {pad_token_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13YlG6psrNjR",
        "outputId": "0c171e2f-468b-46b8-a263-083dc56abfd7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding Token ID: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Padding Token ID: {tokenizer.pad_token_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9ux5qyYqx2n",
        "outputId": "36e6d4a0-1192-47e4-ca07-4763d2d75668"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padding Token ID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n"
      ],
      "metadata": {
        "id": "V49wJmOArEet"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in data_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "\n",
        "        loss = criterion(output.view(-1, output_dim), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n"
      ],
      "metadata": {
        "id": "Lem53mvzrU70"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_px1pjOdrfC4",
        "outputId": "81a2bf31-d8b6-4d1a-b433-25bc39e7d99b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0->torchtext) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0->torchtext) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0->torchtext) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def evaluate_bleu(model, data_loader, device):\n",
        "    model.eval()\n",
        "    candidates, references = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in data_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1]).argmax(dim=-1)\n",
        "            candidates.append(output.tolist())\n",
        "            references.append(tgt.tolist())\n",
        "\n",
        "    return bleu_score(candidates, references)\n"
      ],
      "metadata": {
        "id": "H04_mMc5rWlu"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch torchtext -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty7eGqd9rYal",
        "outputId": "49f16ced-c4bd-46ca-d121-c83748796909"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.0.0\n",
            "Uninstalling torch-2.0.0:\n",
            "  Successfully uninstalled torch-2.0.0\n",
            "Found existing installation: torchtext 0.15.1\n",
            "Uninstalling torchtext-0.15.1:\n",
            "  Successfully uninstalled torchtext-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext==0.15.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkwtjDkBs0YJ",
        "outputId": "78c817e6-2c50-456b-fc53-5df2406b680f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.15.1 in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.32.3)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.1) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (18.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.1) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE96i_R4s59a",
        "outputId": "17ebe586-b419-489f-c465-eb457038299e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.0+cu117\n",
            "0.15.1+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def evaluate_bleu(model, data_loader, device):\n",
        "    model.eval()\n",
        "    candidates, references = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in data_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1]).argmax(dim=-1)\n",
        "            candidates.append(output.tolist())\n",
        "            references.append(tgt.tolist())\n",
        "\n",
        "    return bleu_score(candidates, references)\n"
      ],
      "metadata": {
        "id": "5eHV2Snvs-yF"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NVR3yRmOtz_F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "c2irEVj0uDy2"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Example Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Example Data Loader\n",
        "train_loader = DataLoader(TensorDataset(torch.randn(100, 784), torch.randint(0, 10, (100,))), batch_size=32)\n"
      ],
      "metadata": {
        "id": "7c48QEzjuL2h"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n"
      ],
      "metadata": {
        "id": "OO9QuERGuNd7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, vocab_size):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        src, tgt = batch\n",
        "\n",
        "        # Debugging step to inspect tensor shapes\n",
        "        print(f\"Batch shapes -> src: {src.shape}, tgt: {tgt.shape}\")\n",
        "\n",
        "        # Ensure tensors are at least 2D for shape consistency\n",
        "        if src.dim() == 1:\n",
        "            src = src.unsqueeze(0)  # Convert [seq_len] to [1, seq_len]\n",
        "        if tgt.dim() == 1:\n",
        "            tgt = tgt.unsqueeze(0)\n",
        "\n",
        "        # Handle empty sequences to avoid shape issues\n",
        "        if src.size(1) == 0 or tgt.size(1) == 0:\n",
        "            print(\"Empty batch detected. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Ensure tensors are in correct format and within valid range\n",
        "        src = src.clamp(0, vocab_size - 1).long()\n",
        "        tgt = tgt.clamp(0, vocab_size - 1).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Model expects (src, tgt[:, :-1]) as input and (tgt[:, 1:]) as label\n",
        "        outputs = model(src, tgt[:, :-1])\n",
        "        loss = criterion(outputs, tgt[:, 1:])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "RtH2C1esuTR9"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(loss):\n",
        "    return math.exp(loss)\n"
      ],
      "metadata": {
        "id": "U6lPfc2IuZOz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoY-sL7QudD-",
        "outputId": "55572584-ef55-4cd6-ae91-ffffd32c1f07"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    src, tgt = batch\n"
      ],
      "metadata": {
        "id": "m2jUyEuR4A_z"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "    print(batch)  # Inspect what `batch` contains\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSsbWgT94A9e",
        "outputId": "4d2bba1e-cc15-4bd8-8127-32ffe8cf8bc9"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[-0.7358, -0.9415,  1.0479,  ..., -0.0088, -3.1458,  0.8570],\n",
            "        [-0.2672,  2.2041,  1.1032,  ..., -0.7124, -1.2057, -0.3686],\n",
            "        [-0.9392,  0.2753, -0.5374,  ..., -0.2834, -3.3172,  1.3903],\n",
            "        ...,\n",
            "        [ 0.1872,  1.4039, -0.1594,  ...,  0.4510,  0.0265, -0.5584],\n",
            "        [ 1.4610, -0.1598, -0.5088,  ...,  0.2585,  1.2573, -1.2443],\n",
            "        [ 1.3106, -0.9712, -1.9561,  ...,  0.3724,  0.4874,  1.2768]]), tensor([8, 6, 8, 9, 3, 6, 7, 8, 3, 5, 9, 6, 6, 2, 6, 2, 0, 9, 4, 4, 9, 3, 6, 8,\n",
            "        3, 8, 4, 2, 9, 4, 5, 9])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kh-bVNw84A6s"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TLwPdJHe4A3t"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 3000  # Or set it to your actual vocabulary size\n",
        "epochs = 20  # Define number of epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, vocab_size)\n",
        "\n",
        "    # Gradient Clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    # Learning Rate Scheduler\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {train_loss:.4f}\")\n",
        "\n",
        "# Save the trained model AFTER the training loop\n",
        "torch.save(model.state_dict(), \"translation_model.pt\")\n",
        "print(\"Model saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "87s9KedJufpC",
        "outputId": "9752f32d-19ed-4fc8-d3d9-0c1fcb7055a6"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch shapes -> src: torch.Size([32, 784]), tgt: torch.Size([32])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[32, 248, 32]' is invalid for input of size 6422528",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-61193611f533>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Gradient Clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-89-72a07fc7db13>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion, vocab_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Model expects (src, tgt[:, :-1]) as input and (tgt[:, 1:]) as label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-95698228c562>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    361\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    715\u001b[0m     def _mha_block(self, x: Tensor, mem: Tensor,\n\u001b[1;32m    716\u001b[0m                    attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 717\u001b[0;31m         x = self.multihead_attn(x, mem, mem,\n\u001b[0m\u001b[1;32m    718\u001b[0m                                 \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m                                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1190\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5241\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatic_k\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5243\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5244\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5245\u001b[0m         \u001b[0;31m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 248, 32]' is invalid for input of size 6422528"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Jb3mqGLvyZ9W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8f3oNzYPyiSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def calculate_bleu(references, hypotheses):\n",
        "    score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
        "    return score\n",
        "\n",
        "# Example usage\n",
        "references = [\"this is a test\", \"another sentence example\"]\n",
        "hypotheses = [\"this is a test\", \"another sample sentence\"]\n",
        "print(\"BLEU Score:\", calculate_bleu(references, hypotheses))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqBqdsn3ulR1",
        "outputId": "b3db400c-3480-4ef6-829b-57a20e4cf78e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.8740063190188735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeFXvSAtvcxz",
        "outputId": "e4991788-a97f-4a89-c845-024837417eda"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge(references, hypotheses):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = [scorer.score(ref, hyp) for ref, hyp in zip(references, hypotheses)]\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "references = [\"this is a test\", \"another sentence example\"]\n",
        "hypotheses = [\"this is a test\", \"another sample sentence\"]\n",
        "print(\"ROUGE Scores:\", calculate_rouge(references, hypotheses))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PF8v4ZSvCxW",
        "outputId": "dcdad373-94e2-4eb5-c776-96e00b40177f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores: [{'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def human_evaluation(translations):\n",
        "    scores = []\n",
        "\n",
        "    print(\"Rate each translation on a scale of 1 to 5:\")\n",
        "    for idx, item in enumerate(translations):\n",
        "        print(f\"\\nSample {idx + 1}\")\n",
        "        print(f\"English Sentence: {item['english']}\")\n",
        "        print(f\"Model Translation: {item['model_translation']}\")\n",
        "        print(f\"Reference Translation: {item['reference_translation']}\")\n",
        "\n",
        "        fluency = int(input(\"Fluency (1-5): \"))\n",
        "        adequacy = int(input(\"Adequacy (1-5): \"))\n",
        "        overall_quality = int(input(\"Overall Quality (1-5): \"))\n",
        "\n",
        "        scores.append({\n",
        "            'fluency': fluency,\n",
        "            'adequacy': adequacy,\n",
        "            'overall_quality': overall_quality\n",
        "        })\n",
        "\n",
        "    # Calculate averages\n",
        "    avg_fluency = sum(score['fluency'] for score in scores) / len(scores)\n",
        "    avg_adequacy = sum(score['adequacy'] for score in scores) / len(scores)\n",
        "    avg_quality = sum(score['overall_quality'] for score in scores) / len(scores)\n",
        "\n",
        "    print(\"\\n--- Evaluation Results ---\")\n",
        "    print(f\"Average Fluency: {avg_fluency:.2f}\")\n",
        "    print(f\"Average Adequacy: {avg_adequacy:.2f}\")\n",
        "    print(f\"Average Overall Quality: {avg_quality:.2f}\")\n",
        "\n",
        "# Sample Data\n",
        "evaluation_data = [\n",
        "    {\"english\": \"The weather is nice today.\",\n",
        "     \"model_translation\": \"आज का मौसम अच्छा है।\",\n",
        "     \"reference_translation\": \"आज मौसम अच्छा है।\"},\n",
        "    {\"english\": \"I love programming.\",\n",
        "     \"model_translation\": \"मुझे प्रोग्रामिंग पसंद है।\",\n",
        "     \"reference_translation\": \"मैं प्रोग्रामिंग पसंद करता हूँ।\"}\n",
        "]\n",
        "\n",
        "human_evaluation(evaluation_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpHMzab6vsyv",
        "outputId": "0504763c-7231-4c2a-9990-9d5db702ab9e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate each translation on a scale of 1 to 5:\n",
            "\n",
            "Sample 1\n",
            "English Sentence: The weather is nice today.\n",
            "Model Translation: आज का मौसम अच्छा है।\n",
            "Reference Translation: आज मौसम अच्छा है।\n",
            "Fluency (1-5): 3\n",
            "Adequacy (1-5): 4\n",
            "Overall Quality (1-5): 5\n",
            "\n",
            "Sample 2\n",
            "English Sentence: I love programming.\n",
            "Model Translation: मुझे प्रोग्रामिंग पसंद है।\n",
            "Reference Translation: मैं प्रोग्रामिंग पसंद करता हूँ।\n",
            "Fluency (1-5): 2\n",
            "Adequacy (1-5): 3\n",
            "Overall Quality (1-5): 4\n",
            "\n",
            "--- Evaluation Results ---\n",
            "Average Fluency: 2.50\n",
            "Average Adequacy: 3.50\n",
            "Average Overall Quality: 4.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_model.model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlvrmF64wGh_",
        "outputId": "be6c669e-1c8b-4f5c-c3da-5436e93b8e74"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Load SentencePiece tokenizer\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"spm_model.model\")\n",
        "\n",
        "# Load your trained model\n",
        "class TranslationModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(TranslationModel, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = torch.nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, _ = self.lstm(embedded)\n",
        "        return self.fc(output)\n",
        "\n",
        "vocab_size = len(sp)\n",
        "model = TranslationModel(vocab_size, 256, 512)\n",
        "model.load_state_dict(torch.load(\"translation_model.pt\"), strict=False)\n",
        "model.eval()\n",
        "\n",
        "# Translation function\n",
        "def translate(sentence):\n",
        "    tokens = sp.encode(sentence, out_type=int)\n",
        "    input_tensor = torch.tensor([tokens]).long()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "\n",
        "    translated_tokens = torch.argmax(output, dim=-1).squeeze().tolist()\n",
        "    translated_sentence = sp.decode(translated_tokens)\n",
        "    return translated_sentence\n",
        "\n",
        "# Test the translation\n",
        "sentence = \"The sun rises in the east.\"\n",
        "print(\"Translated Sentence:\", translate(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "UvsVL0Ozwa5d",
        "outputId": "1a72512f-35fc-46b4-cdd0-0c2fbfe06af4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for TranslationModel:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a39a7464f0e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTranslationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"translation_model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TranslationModel:\n\tMissing key(s) in state_dict: \"embedding.weight\", \"lstm.weight_ih_l0\", \"lstm.weight_hh_l0\", \"lstm.bias_ih_l0\", \"lstm.bias_hh_l0\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdLfbxq9xJ7F",
        "outputId": "dacee92c-59f3-4183-adb3-994deba081a0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'train_data.csv', 'tokenized_file.csv', 'train_tokenized.csv', 'augmented_data.csv', '.ipynb_checkpoints', 'spm_model.model', 'cleaned_sentences.csv', 'test_data.csv', 'val_data.csv', 'eng-hin.csv', 'test_tokenized.csv', 'alignment_issues.csv', 'val_tokenized.csv', 'spm_model.vocab', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "import ast\n",
        "\n",
        "# Define Dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = pd.read_csv(file_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = torch.tensor(ast.literal_eval(self.data.iloc[idx, 0]))\n",
        "        tgt = torch.tensor(ast.literal_eval(self.data.iloc[idx, 1]))\n",
        "        return src, tgt\n",
        "\n",
        "# Translation Model\n",
        "class TranslationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super(TranslationModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        _, (hidden, _) = self.encoder(embedded)\n",
        "        output = self.decoder(hidden.squeeze(0))\n",
        "        return output\n",
        "\n",
        "# Load SentencePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_model.model\")\n",
        "\n",
        "# Load Model\n",
        "vocab_size = len(sp)\n",
        "model = TranslationModel(vocab_size, 256, 512)\n",
        "\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"translation_model.pt\"))\n",
        "    print(\"Model loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model file not found. Please train and save the model first.\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def translate_sentence(sentence):\n",
        "    tokens = sp.encode(sentence, out_type=str)\n",
        "    token_ids = torch.tensor(sp.encode(sentence)).unsqueeze(0)  # Add batch dim\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(token_ids)\n",
        "        predicted_ids = torch.argmax(output, dim=-1).squeeze().tolist()\n",
        "\n",
        "    translated_sentence = sp.decode(predicted_ids)\n",
        "    return translated_sentence\n",
        "\n",
        "# Testing the model with sample sentence\n",
        "test_sentence = \"The sun rises in the east.\"\n",
        "print(f\"English: {test_sentence}\")\n",
        "print(f\"Hindi Translation: {translate_sentence(test_sentence)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUi3SO3gxP_m",
        "outputId": "424f9afa-c18c-4f9f-b56e-a685ab5e6e07"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Model file not found. Please train and save the model first.\n",
            "English: The sun rises in the east.\n",
            "Hindi Translation: बिल\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gw3ceN9LxY_7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}