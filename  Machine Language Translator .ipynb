{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saarang2003/Machine-Tranlator/blob/main/%20Machine%20Language%20Translator%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git\n"
      ],
      "metadata": {
        "id": "uqXyxz00SkCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ],
      "metadata": {
        "id": "yvpjkS0UyJ0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "VJrdaw3VyJ29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEThvD78yVEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None  # Options: None, \"4-bit\", or \"8-bit\"\n",
        "\n",
        "# Function to initialize model and tokenizer\n",
        "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
        "    if quantization == \"4-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif quantization == \"8-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qconfig = None\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ckpt_dir,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qconfig,\n",
        "    )\n",
        "\n",
        "    if qconfig is None:\n",
        "        model = model.to(DEVICE)\n",
        "        if DEVICE == \"cuda\":\n",
        "            model.half()\n",
        "\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            decoded = tokenizer.batch_decode(\n",
        "                generated_tokens.detach().cpu().tolist(),\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "\n",
        "        translations += ip.postprocess_batch(decoded, lang=tgt_lang)\n",
        "        del inputs\n",
        "        if DEVICE == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "    return translations\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_model():\n",
        "    ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"\n",
        "    tokenizer, model = initialize_model_and_tokenizer(ckpt_dir, quantization)\n",
        "    ip = IndicProcessor(inference=True)\n",
        "    return tokenizer, model, ip\n",
        "\n",
        "def main():\n",
        "    st.title(\"IndicTrans2 English-to-Indic Translation\")\n",
        "    st.write(\"Enter English sentences and get translations in your desired Indic language.\")\n",
        "\n",
        "    st.sidebar.header(\"Translation Settings\")\n",
        "    src_lang = st.sidebar.selectbox(\"Source Language\", options=[\"eng_Latn\"], index=0)\n",
        "    target_options = {\n",
        "        \"Assamese\": \"asm_Beng\",\n",
        "        \"Bengali\": \"ben_Beng\",\n",
        "        \"Gujarati\": \"guj_Gujr\",\n",
        "        \"Hindi\": \"hin_Deva\",\n",
        "        \"Kannada\": \"kan_Knda\",\n",
        "        \"Malayalam\": \"mal_Mlym\",\n",
        "        \"Marathi\": \"mar_Deva\",\n",
        "        \"Nepali\": \"npi_Deva\",\n",
        "        \"Oriya\": \"ori_Orya\",\n",
        "        \"Punjabi\": \"pan_Guru\",\n",
        "        \"Tamil\": \"tam_Taml\",\n",
        "        \"Telugu\": \"tel_Telu\",\n",
        "        \"Urdu\": \"urd_Arab\",\n",
        "    }\n",
        "    tgt_lang_name = st.sidebar.selectbox(\"Target Language\", list(target_options.keys()))\n",
        "    tgt_lang = target_options[tgt_lang_name]\n",
        "\n",
        "    input_text = st.text_area(\"Enter English sentences (one per line)\", height=200)\n",
        "    if st.button(\"Translate\"):\n",
        "        if not input_text.strip():\n",
        "            st.warning(\"Please enter some text to translate.\")\n",
        "        else:\n",
        "            input_sentences = [line.strip() for line in input_text.split(\"\\n\") if line.strip()]\n",
        "            with st.spinner(\"Loading model and translating...\"):\n",
        "                tokenizer, model, ip = load_model()\n",
        "                translations = batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\n",
        "            st.subheader(\"Translations\")\n",
        "            for src, tgt in zip(input_sentences, translations):\n",
        "                st.markdown(f\"**English:** {src}\")\n",
        "                st.markdown(f\"**{tgt_lang_name}:** {tgt}\")\n",
        "                st.write(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Lz3Pjm0CFtsf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6116d206-4f47-42b3-a882-46203e3f0798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&\n"
      ],
      "metadata": {
        "id": "RA6HGaQ82t_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUB9sVUWyJ6W",
        "outputId": "300fac58-c861-47f0-8272-4b66dae7f1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.41.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnMr8bxI0n4B",
        "outputId": "344aaac0-bcb7-4a14-f5ad-e80cb25ab1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "VAuwoYkg3NCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok authtoken (replace with your actual token)\n",
        "ngrok.set_auth_token(\"2saHaLzNEOiPkOpNnWLBRe0P6KZ_efXapqC6vMMNp9iKRrR5\")\n",
        "\n",
        "# Optionally, kill any existing tunnels\n",
        "# ngrok.kill()\n",
        "\n",
        "# Create a tunnel to the Streamlit port (8501 by default)\n",
        "public_url = ngrok.connect(8501, \"http\", bind_tls=False)\n",
        "print(f\"Streamlit app is available at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQTUxa_8zaCM",
        "outputId": "e2ab0125-916a-43a5-e2ec-803cf06101b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is available at: NgrokTunnel: \"http://1c27-35-196-201-177.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "streamlit run app.py\n"
      ],
      "metadata": {
        "id": "2m2f1xIqyrNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&\n"
      ],
      "metadata": {
        "id": "czpKMtMy1f9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QTvqbQAM3A8i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "IB2KA9pUSGY8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n"
      ],
      "metadata": {
        "id": "OqrTkbetSL0D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Example input lists\n",
        "# hindi_sentences = [...]  # Already cleaned and aligned\n",
        "# english_sentences = [...]  # Already cleaned and aligned\n",
        "\n",
        "# Tokenizers\n",
        "hin_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "eng_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
        "\n",
        "hin_tokenizer.fit_on_texts(hindi_sentences)\n",
        "eng_tokenizer.fit_on_texts(english_sentences)\n",
        "\n",
        "# Convert sentences to sequences\n",
        "hindi_sequences = hin_tokenizer.texts_to_sequences(hindi_sentences)\n",
        "english_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# ✅ Remove empty sequences (avoid pad_sequences error)\n",
        "filtered_pairs = [(hin, eng) for hin, eng in zip(hindi_sequences, english_sequences) if len(hin) > 0 and len(eng) > 1]\n",
        "\n",
        "hindi_sequences = [pair[0] for pair in filtered_pairs]\n",
        "english_sequences = [pair[1] for pair in filtered_pairs]\n",
        "\n",
        "# Recalculate max lengths\n",
        "max_len_hin = max(len(seq) for seq in hindi_sequences)\n",
        "max_len_eng = max(len(seq) for seq in english_sequences)\n",
        "\n",
        "# Pad encoder input\n",
        "encoder_input_data = pad_sequences(hindi_sequences, maxlen=max_len_hin, padding='post')\n",
        "\n",
        "# Prepare decoder input and target\n",
        "decoder_input_sequences = [seq[:-1] for seq in english_sequences]\n",
        "decoder_target_sequences = [seq[1:] for seq in english_sequences]\n",
        "\n",
        "decoder_input_data = pad_sequences(decoder_input_sequences, maxlen=max_len_eng-1, padding='post')\n",
        "decoder_target_data = pad_sequences(decoder_target_sequences, maxlen=max_len_eng-1, padding='post')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "f9_0QAqWSZop",
        "outputId": "330fecee-89bb-4dd5-89b8-8ef8ae4dd4fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hindi_sentences' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5fb25ea2baa4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0meng_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<OOV>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhin_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhindi_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0meng_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hindi_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp_eng = spm.SentencePieceProcessor()\n",
        "sp_eng.load('eng_spm.model')\n",
        "\n",
        "sp_hin = spm.SentencePieceProcessor()\n",
        "sp_hin.load('hin_spm.model')  # or hin_spm.model if you used SentencePiece for Hindi too\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofcSHdlfSklE",
        "outputId": "909c61e4-ec27-4f32-8619-a7154c2f58bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read your train.eng and train.hin files as lists of sentences\n",
        "with open('train.eng', 'r', encoding='utf-8') as f:\n",
        "    train_eng_text = f.readlines()\n",
        "\n",
        "with open('train.hin', 'r', encoding='utf-8') as f:\n",
        "    train_hin_text = f.readlines()\n",
        "\n"
      ],
      "metadata": {
        "id": "ItqTx3EhUFeM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eng_ids = [sp_eng.encode_as_ids(sentence.strip()) for sentence in train_eng_text]\n",
        "train_hin_ids = [sp_hin.encode_as_ids(sentence.strip()) for sentence in train_hin_text]\n"
      ],
      "metadata": {
        "id": "d7nLmYn4ULgj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len_eng = max(len(seq) for seq in train_eng_ids)\n",
        "max_len_hin = max(len(seq) for seq in train_hin_ids)\n",
        "\n",
        "encoder_input_data = pad_sequences(train_hin_ids, maxlen=max_len_hin, padding='post')\n",
        "decoder_input_data = pad_sequences(train_eng_ids, maxlen=max_len_eng, padding='post')\n"
      ],
      "metadata": {
        "id": "qkk2Ge7EU7es"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data = np.array([seq[1:] + [0] for seq in decoder_input_data])\n",
        "decoder_target_data = pad_sequences(decoder_target_data, maxlen=max_len_eng, padding='post')\n",
        "decoder_target_data = np.expand_dims(decoder_target_data, -1)\n"
      ],
      "metadata": {
        "id": "FgxT7N20VAnQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hin_vocab_size = sp_hin.get_piece_size()\n",
        "eng_vocab_size = sp_eng.get_piece_size()\n"
      ],
      "metadata": {
        "id": "z9iTCctCVCiN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='train.eng',\n",
        "    model_prefix='eng_spm',\n",
        "    vocab_size=8000,   # You can set 4000/5000/8000 as needed\n",
        "    model_type='bpe',  # or 'unigram'\n",
        "    character_coverage=1.0  # For English, 1.0 is okay\n",
        ")\n"
      ],
      "metadata": {
        "id": "rMW_Y4MuVWUq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train(\n",
        "    input='train.hin',\n",
        "    model_prefix='hin_spm',\n",
        "    vocab_size=8000,\n",
        "    model_type='bpe',\n",
        "    character_coverage=0.9995  # For Hindi/Devanagari, slightly less\n",
        ")\n"
      ],
      "metadata": {
        "id": "MEtmTMjtVX7c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_eng = spm.SentencePieceProcessor()\n",
        "sp_eng.load(\"eng_spm.model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV0ogspLWxSe",
        "outputId": "d7b6c909-d9a2-405b-9ad4-f38b8d9ce02e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_eng = spm.SentencePieceProcessor()\n",
        "sp_eng.load(\"hin_spm.model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MuzcOb8W0Fl",
        "outputId": "a963ccbf-42f5-4a9e-c0fa-d08e2c2f3b71"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read lines from train files\n",
        "with open('train.eng', 'r', encoding='utf-8') as f:\n",
        "    train_eng_text = f.readlines()\n",
        "\n",
        "with open('train.hin', 'r', encoding='utf-8') as f:\n",
        "    train_hin_text = f.readlines()\n"
      ],
      "metadata": {
        "id": "vGwsWYHMW1_Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode text into token IDs\n",
        "train_eng_ids = [sp_eng.encode_as_ids(sentence.strip()) for sentence in train_eng_text]\n",
        "train_hin_ids = [sp_hin.encode_as_ids(sentence.strip()) for sentence in train_hin_text]\n"
      ],
      "metadata": {
        "id": "XTBfmXCWXIJX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len_eng = max(len(seq) for seq in train_eng_ids)\n",
        "max_len_hin = max(len(seq) for seq in train_hin_ids)\n",
        "\n",
        "train_eng_padded = pad_sequences(train_eng_ids, maxlen=max_len_eng, padding='post')\n",
        "train_hin_padded = pad_sequences(train_hin_ids, maxlen=max_len_hin, padding='post')\n"
      ],
      "metadata": {
        "id": "QGQ9zGk1XKIl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "decoder_input = [seq[:-1] for seq in train_hin_ids]\n",
        "decoder_output = [seq[1:] for seq in train_hin_ids]\n",
        "\n",
        "decoder_input_padded = pad_sequences(decoder_input, maxlen=max_len_hin-1, padding='post')\n",
        "decoder_output_padded = pad_sequences(decoder_output, maxlen=max_len_hin-1, padding='post')\n",
        "\n",
        "encoder_input = np.array(train_eng_padded)\n",
        "decoder_input = np.array(decoder_input_padded)\n",
        "decoder_output = np.array(decoder_output_padded)\n"
      ],
      "metadata": {
        "id": "wjKLihiMXLb_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "\n",
        "# ----------- STEP 1: Load SentencePiece Tokenizers ------------\n",
        "sp_eng = spm.SentencePieceProcessor()\n",
        "sp_eng.load(\"eng_spm.model\")\n",
        "\n",
        "sp_hin = spm.SentencePieceProcessor()\n",
        "sp_hin.load(\"hin_spm.model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO4QsariXPr3",
        "outputId": "6374264d-5451-4bed-90af-8e43354c7200"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- STEP 2: Load and Encode Training Data ------------\n",
        "with open(\"train.eng\", encoding=\"utf-8\") as f:\n",
        "    train_eng = [sp_eng.encode_as_ids(line.strip()) for line in f.readlines()]\n",
        "\n",
        "with open(\"train.hin\", encoding=\"utf-8\") as f:\n",
        "    train_hin = [sp_hin.encode_as_ids(line.strip()) for line in f.readlines()]\n",
        "\n",
        "# Add special tokens for BOS and EOS\n",
        "BOS_ID = sp_hin.bos_id() if sp_hin.bos_id() != -1 else 1\n",
        "EOS_ID = sp_hin.eos_id() if sp_hin.eos_id() != -1 else 2\n",
        "\n",
        "encoder_input = tf.keras.preprocessing.sequence.pad_sequences(train_eng, padding='post')\n",
        "decoder_input = tf.keras.preprocessing.sequence.pad_sequences([[BOS_ID]+s for s in train_hin], padding='post')\n",
        "decoder_target = tf.keras.preprocessing.sequence.pad_sequences([s+[EOS_ID] for s in train_hin], padding='post')\n",
        "\n",
        "# ----------- STEP 3: Define Hyperparameters ------------\n",
        "VOCAB_SIZE_EN = sp_eng.get_piece_size()\n",
        "VOCAB_SIZE_HI = sp_hin.get_piece_size()\n",
        "EMBEDDING_DIM = 256\n",
        "UNITS = 512\n"
      ],
      "metadata": {
        "id": "F_H32LNTXakH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- STEP 4: Build Encoder ------------\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(enc_units, return_sequences=True, return_state=True)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        return output, state_h, state_c\n"
      ],
      "metadata": {
        "id": "b3fZnNO6XcsE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, enc_output, dec_hidden):\n",
        "        # dec_hidden shape: (batch, hidden)\n",
        "        dec_hidden_with_time_axis = tf.expand_dims(dec_hidden, 1)\n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(dec_hidden_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "VkigQpVhXels"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- STEP 6: Build Decoder ------------\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(dec_units, return_sequences=True, return_state=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = Attention(dec_units)\n",
        "\n",
        "    def call(self, x, enc_output, hidden):\n",
        "        context_vector, attention_weights = self.attention(enc_output, hidden)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state_h, state_c, attention_weights\n"
      ],
      "metadata": {
        "id": "tWgwNXjdXgRk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- STEP 7: Instantiate and Compile ------------\n",
        "encoder = Encoder(VOCAB_SIZE_EN, EMBEDDING_DIM, UNITS)\n",
        "decoder = Decoder(VOCAB_SIZE_HI, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "# ----------- STEP 8: Define Loss & Optimizer ------------\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.not_equal(real, 0)\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    return tf.reduce_mean(loss_ * mask)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "rdTb9Bz2XiQU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights\n"
      ],
      "metadata": {
        "id": "36k5Ep7saDoy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- STEP 9: Training Step ------------\n",
        "@tf.function\n",
        "def train_step(enc_inp, dec_inp, dec_target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden, enc_cell = encoder(enc_inp)\n",
        "        dec_hidden, dec_cell = enc_hidden, enc_cell\n",
        "        dec_input = tf.expand_dims(dec_inp[:, 0], 1)\n",
        "\n",
        "        loss = 0\n",
        "        for t in range(1, dec_target.shape[1]):\n",
        "            predictions, dec_hidden, dec_cell, _ = decoder(dec_input, enc_output, dec_hidden)\n",
        "            loss += loss_function(dec_target[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(dec_inp[:, t], 1)\n",
        "\n",
        "    batch_loss = loss / int(dec_target.shape[1])\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "n1iCSvD4Xj0n"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- STEP 10: Train Model ------------\n",
        "BATCH_SIZE = 32\n",
        "dataset = tf.data.Dataset.from_tensor_slices((encoder_input, decoder_input, decoder_target))\n",
        "dataset = dataset.shuffle(len(encoder_input)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for (batch, (enc_inp, dec_inp, dec_target)) in enumerate(dataset):\n",
        "        batch_loss = train_step(enc_inp, dec_inp, dec_target)\n",
        "        total_loss += batch_loss\n",
        "    print(f\"Epoch {epoch+1} Loss {total_loss.numpy() / (batch+1):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5M4fVFxXmPD",
        "outputId": "104f3acd-15e4-45f5-9333-156ba5077a8e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss 1.8573\n",
            "Epoch 2 Loss 1.8447\n",
            "Epoch 3 Loss 1.8313\n",
            "Epoch 4 Loss 1.8180\n",
            "Epoch 5 Loss 1.8044\n",
            "Epoch 6 Loss 1.7909\n",
            "Epoch 7 Loss 1.7783\n",
            "Epoch 8 Loss 1.7652\n",
            "Epoch 9 Loss 1.7506\n",
            "Epoch 10 Loss 1.7365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [sp_eng.encode_as_ids(sentence)]\n",
        "    max_length_input = 40\n",
        "    units = 256\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_input, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_output):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += target_tokenizer.index_word.get(predicted_id, '') + ' '\n",
        "\n",
        "        if target_tokenizer.index_word.get(predicted_id, '') == '<end>':\n",
        "            break\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "VFt3GVgxXnzd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"I love Indian food.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "K6gN3H3-ZcaS",
        "outputId": "e683ab23-b21f-4415-9d1c-b7bbddbfb276"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "too many positional arguments",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-53b25f062dab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I love Indian food.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-151895d48b23>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \"\"\"\n\u001b[0;32m-> 3195\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3114\u001b[0m                     \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'too many positional arguments'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_VAR_KEYWORD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEYWORD_ONLY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: too many positional arguments"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower().strip()\n",
        "    # Add space between words and punctuation\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "DJ-mSIrmZeQi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate(\"I love Indian food.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "b_fMr4Q0ZiS2",
        "outputId": "091c5b1a-626c-4691-d0b7-bd3be6b41fa3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'max_length_input' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-4782ac206020>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I love Indian food.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The weather is nice today.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-454d1b6397b9>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msp_eng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_as_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'max_length_input' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0WNG--vZkMf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}